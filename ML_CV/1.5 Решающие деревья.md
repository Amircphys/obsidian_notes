
**Decision trees**

1. Решающее дерево предсказывает значение целевой переменной с помощью применения последовательности простых решающих правил (которые называются предикатами).
2. Решающие деревья часто используют как кирпичики для построения ансамблей — моделей, делающих предсказания на основе агрегации предсказаний других моделей.
3. Решение о том, к какому классу будет отнесён текущий объект выборки, будет приниматься с помощью прохода от корня дерева к некоторому листу.
4. В каждом узле этого дерева находится предикат. Если предикат верен для текущего примера из выборки, мы переходим в правого потомка, если нет — в левого.
5.  Пример предиката — это просто взятие порога по значению какого-то признака.
6.  В листьях записаны предсказания (например, метки классов). Как только мы дошли до листа, мы присваиваем объекту ответ, записанный в вершине.
7.  Каждый предикат порождает разделение текущего подмножества пространства признаков на две части.
8.  Каждой внутренней вершине $v$ приписан предикат $B_v: \mathbb{X} \to {0,1}$
9.  Каждой листовой вершине $v$ приписан прогноз $c_v \in \mathbb{Y}$, где $\mathbb{Y}$ — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов).
10.  В ходе предсказания осуществляется проход по этому дереву к некоторому листу, движение начинается из корня.  В очередной внутренней вершине  $v$ проход продолжится вправо  если значение предиката равно 1 или влево если 0.
11. Проход продолжается до момента, пока не будет достигнут некоторый лист, и ответом алгоритма на объекте $x$ считается прогноз $c_v$, приписанный этому листу.
12. Предикат может иметь, вообще говоря, произвольную структуру, но, как правило, на практике используют просто сравнение с порогом $t \in \mathbb{R}$ по какому-то $j$-му признаку: $B_v(x, j, t) = [ x_j \le t ]$
13. Выученная функция является кусочно-постоянной, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть;
14. дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки;  древесные модели неспособны к экстраполяции
15. дерево решений способно идеально приблизить обучающую выборку и ничего не выучить
16. значений порога $t$, при которых меняется значение предиката, может быть не более $N-1$
17. Поиск оптимального, с точки зрения качества на обучающей выборке, дерева является NP-полной задачей, выход - разрешить себе искать не оптимальное решение, а просто достаточно хорошее.
18.  Строить дерево с помощью жадного алгоритма, то есть не искать всю структуру сразу, а строить дерево этаж за этажом.
19.  Критерий остановки,например - остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много. 
20.  Критерий ветвления - это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина является листом. 
21. Константный таргет в листе должен минимизировать среднее значение функции потерь, оптимальное значение  функции потерь - информативностью, или impurity. Чем она ниже, тем лучше объекты в листе можно приблизить константным значением. impurity: $$H(X_m) = \min\limits_{c \in Y} \frac{1}{\vert X_m\vert}\sum\limits_{(x_i, y_i) \in X_m} L(y_i, c)$$
22. Критерий ветвления - разность информативности исходной вершины и решающего пня равна: $$\color{#348FEA}{Branch (X_m, j, t) = |X_m| \cdot H(X_m) -  |X_l| \cdot H(X_l) -  |X_r| \cdot H(X_r)}$$
23. При жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист.
24. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше.
25. классификации: misclassification error, оптимальным предсказанием в листе будет наиболее частотный класс
26. Если же мы собрались предсказывать вероятностное распределение классов $(c_1, \ldots, c_K)$, то можно подойти к задаче через максимизацию логарифма правдоподобия
27. Тогда мы в качестве информативности получим энтропию распределения классов $\color{#348FEA}{H(X_m) = -\sum_{k = 1}^K p_k \log p_k}$
28. Энтропи измеряет непредсказуемость реализации случайной величины, если случайная величина принимает только одно значение, то она абсолютно предсказуема и её энтропия равна 0. Наибольшего значения энтропия достигает для равномерно распределённой случайной величины
29. Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, метрику Бриера (за которой стоит всего лишь идея посчитать MSE от вероятностей).
30. Критерий Джини: $\color{#348FEA}{H(X_m) = \sum_{k = 1}^K p_k (1 - p_k)}$ -  равно математическому ожиданию числа неправильно классифицированных объектов в случае, если мы будем приписывать им случайные метки из дискретного распределения, заданного вероятностями $(p_1, \ldots, p_k)$
31. Максимизацию функционала Джини можно условно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве
32. Учет категориальных признаков: разбивать  вершину на столько поддеревьев, сколько имеется возможных значений у признака (multi-way splits) - риск получения дерева с крайне большим числом листьев
33. Разобиваем множество значений на два  непересекающихся подмножества, и определим предикат как индикатор попадания в первое подмножество. Основная проблема заключается в том, что для построения оптимального предиката нужно перебирать $~2^{q}$ вариантов разбиения, что может быть не вполне возможным.
34. Можно обойтись без полного перебора в случаях с бинарной классификацией и регрессией. В случае с бинарной классификацией упорядочим все значения категориального признака на основе того, какая доля объектов с таким значением имеет класс +1. после чего заменим категорию $u(i)$ на число i, и будем искать разбиение как для вещественного признака.
35. Для задачи регрессии с MSE-функционалом это тоже будет верно, если упорядочивать значения признака по среднему ответу объектов с таким значением.
36. Пропуски - в момент выбора сплитов  по признаку  мы будет просто игнорировать объекты с пропусками, а когда сплит выбран, мы отправим их в оба поддерева.
37. При этом логично присвоить им веса в функции потерь: $\frac{\vert X_l\vert}{\vert X_m\vert}$ для левого поддерева, $\frac{\vert X_r\vert}{\vert X_m\vert}$ для правого.
38. На этапе применения дерева отправляем объект и в левое и правое поддеревья и получаем предсказания  $y_l$, $y_r$. Эти предсказания мы усредним с весами $\frac{\vert X_l\vert}{\vert X_m\vert}$ и $\frac{\vert X_r\vert}{\vert X_m\vert}$
39. Другой подход заключается в построении суррогатных предикатов в каждой вершине. Так называется предикат, который использует другой признак, но при этом дает разбиение, максимально близкое к данному
40. Если речь идёт о категориальном признаке, может оказаться хорошей идеей ввести дополнительное значение «пропущено» для категориального признака и дальше работать с пропусками, как с обычным значением
41. Методы регуляризации: ограничение по максимальной глубине дерева; ограничение на минимальное количество объектов в листе; ограничение на максимальное количество листьев в дереве; требование, чтобы функционал качества  при делении текущей подвыборки на две улучшался не менее чем на $s$%.