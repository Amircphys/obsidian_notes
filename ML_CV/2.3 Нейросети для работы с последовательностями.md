Для работы с текстовыми данными есть несколько возможных режимов:

1. **Many-to-one**. На вход подается последовательность объектов, на выходе один объект. Пример 2: классификация текстов или видео. Пример 2: тематическая классификация. По предложению нефиксированной длины генерируем вектор вероятностей упоминания заранее фиксированных тем во входном предложении. Размерность выходного вектора постоянна и равна количеству тем.
2. **One-to-many**. На вход подается один объект, на выходе последовательность объектов. Пример: генерация заголовка к изображению (image captioning).
3. **Many-to-many**. На входе и выходе последовательности нефиксированной длины. Примеры: машинный перевод, суммаризация текста, генерация заголовка к статье.
4. **Синхронизированный вариант many-to-many**. На входе и выходе последовательности одинаковой длины, токены входной явно сопоставлены соответствующим токенам выходной. Пример: генерация покадровых субтитров к видео, PoS-tagging (part of speech tagging, для каждого слова в предложении предсказываем, что это за часть речи).
![[Pasted image 20250701091200.png]]
Word2vec впервые он был предложен Т.Миколовым в 2013 году в [статье](https://arxiv.org/abs/1301.3781) Efficient Estimation of Word Representations in Vector Space.
Для обучения авторы предложили две стратегии: **Skip-gram** и **CBOW** (Сontinuous bag-of-words):
- В архитектуре CBOW модель учится предсказывать данное (центральное) слово по контексту (например, по двум словам перед данным и двум словам после него).
- В архитектуре Skip-gram модель учится по слову предсказывать контекст (например, каждого из двух соседей слева и справа)

![[Pasted image 20250701091428.png]]
![[Pasted image 20250701091527.png]]
### LSTM

Вдохновение при написании этого параграфа черпалось из [статьи](https://colah.github.io/posts/2015-08-Understanding-LSTMs) в блоге исследователя Кристофера Олаха, из него же взяты иллюстрации.

Сеть с долговременной и кратковременной памятью (Long short term memory, LSTM) частично решает проблему исчезновения или зашкаливания градиентов в процессе обучения рекуррентных сетей методом обратного распространения ошибки. Эта архитектура была [предложена](http://www.bioinf.jku.at/publications/older/2604.pdf) Hochreiter & Schmidhuber в 1997 году. LSTM построена таким образом, чтобы учитывать долговременные зависимости. Все рекуррентные сети можно представить в виде цепочки из повторяющихся блоков. В RNN таким блоком обычно является один линейный слой с гиперболическим тангенсом в качестве функции активации. В LSTM повторяющийся блок имеет более сложную структуру, состоящую не из одного, а из четырех слоев. Кроме скрытого состояния $h_n$​, в LSTM появляется понятие состояния блока (cell state, $c_n$​). 

Cell state $c_n$ будет играть роль внутренней, закрытой информации LSTM-блока, тогда как скрытое состояние $h_n$ теперь становится передаваемым наружу (не только в следующий блок, но и на следующий слой или выход всей сети) значением. LSTM может добавлять или удалять определенную информацию из cell state с помощью специальных механизмов, которые называются **gates** (ворота или вентили в русскоязычной литературе).
Основное назначение вентиля — контролировать количество проходящей через него информации. Для этого матрица, проходящая по каналу, который контролирует вентиль, поточечно умножается на выражение вида

$$σ(W_1h_{n−1}+W_2x_n)$$
Сигмоида выдает значение от 0 до 1. Оно означает, какая доля информации сможет пройти через вентиль. Рассмотрим типы гейтов в том порядке, в каком они применяются в LSTM.
**Forget gate** (вентиль забывания). Он позволяет на основе предыдущего скрытого состояния $h_{t−1}$​ и нового входа $x_t$ определить, какую долю информации из $c_{t−1}$ (состояния предыдущего блока) стоит пропустить дальше, а какую забыть.
![[Pasted image 20250701092405.png]]
#### Gated Recurrent Unit (GRU)

Gated Recurrent Unit был предложен в [статье](https://arxiv.org/pdf/1406.1078v3.pdf) Cho et al. в 2014 году. GRU объединяет input gate и forget gate в один **update gate**, также устраняет разделение внутренней информации блока на hidden и cell state. Вот общий вид GRU-блока: ![[Pasted image 20250701092558.png]]
## Seq2seq
Действительно: имевшиеся у нас пока инструменты не позволяли генерировать последовательности произвольной длины. Но как тогда переводить с одного языка на другой? Ведь мы не знаем, какой должна быть длина перевода фразы, да и однозначного соответствия между словами исходного предложения и его перевода обычно нет.

Естественным решением для задачи sequence-to-sequence (seq2seq) является использование архитектуры **энкодер-декодер**, состоящей из кодировщика (**энкодера**) для кодирования информации об исходной последовательности в **контекстном векторе** (context vector) и декодировщика (**декодера**) для превращения закодированной энкодером информации в новую последовательность.
