
1. На этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают **offline** и **online** (**оффлайновыми** и **онлайновыми**). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров.
2.  методы обучения реализуют разные подходы к обучению:
	- обучение на основе прироста информации (как в деревьях решений);
	- обучение на основе сходства (как в методах ближайших соседей);
	- обучение на основе вероятностной модели данных (например, максимизацией правдоподобия);
	- обучение на основе ошибок (минимизация эмпирического риска).
3. Важно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом:
	- Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью).    
	- Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток. 
4. **доля ошибочных классификаций** (**error rate**): Error rate=1−Accuracy
5. Исторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей).
6. Две задачи:
		- Объекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные. В данной задаче очень важной характеристикой является FN  - доля злокачественных опухолей, которые классификатор пропускает, ведь каждая не обнаруженная опухоль может стоить человеческой жизни.
		- по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее.
7. ![[Pasted image 20250415174313.png]]
8. **Recall@k, Precision@k** - в выдаче D размера k по запросу q нашлись релевантные документы
9. **AUC**. 
	- **TPR** (**true positive rate**) — это полнота, доля положительных объектов, правильно предсказанных положительными: TPR=TP/P
	- **FPR** (**false positive rate**) — это доля отрицательных объектов, неправильно предсказанных положительными: FPR=FP/N
	Обе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется **ROC-кривой** (**receiver operating characteristics curve**, сокращённо **ROC curve**). ![[Pasted image 20250415175540.png]]
	Если классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1), площадь под которой равна 1.
    
	Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5.
	Чем лучше классификатор разделяет два класса, тем больше площадь (_area under curve_) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется **AUC** и она работает благодаря следующему свойству ROC-кривой:
	
	**AUC** равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше:
10. В каких случаях лучше отдать предпочтение **AUC**? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и 0 — иначе. Однако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях. **Таким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC.**
11. **AUC в задачах ранжирования следует использовать с осторожностью**. ![[Pasted image 20250415180659.png]]
12. **Average Precision** - Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 00 до 11, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет).

	 Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида:
		![[Pasted image 20250415181543.png]]
	Площади под этой кривой - **average precision**
13. **Многоклассовая классификация**. 
    -  **микроусреднение** - усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру (делает вклад маленького класса в общую метрику незаметным).
    - **макроусреднение** - Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняе (вклад каждого класса будет одинаковым)
14. Метрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах.
15. **Как оптимизировать метрики классификации?** ![[Pasted image 20250415183302.png]]
16. 