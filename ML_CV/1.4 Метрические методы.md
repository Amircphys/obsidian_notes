

Алгоритмы этого класса почти не имеют фазы обучения. Вместо этого они просто запоминают всю обучающую выборку, а на этапе предсказания просто ищут объекты, похожие на целевой. Такой процесс называют **lazy learning**, потому что никакого обучения, по сути, не происходит.

 Также метрические модели являются непараметрическими, потому что они не делают явных допущений о глобальных законах, которым подчиняются данные. Так, линейная регрессия основывается на предположении о том, что изучаемая закономерность линейная (с неизвестными коэффициентами, которые восстанавливаются по выборке), а линейная бинарная классификация — что существует гиперплоскость, неплохо разделяющая классы. Метрические методы же локальны: они исходят из допущения, что свойства объекта можно узнать, имея представление о его соседях.

## Метод k-ближайших соседей (KNN)
Этот подход в основном чисто инженерный из-за отсутствия фазы обучения — в настоящее время уже почти нигде не применяется. Однако многие техники, на которых основан алгоритм, используются и в других методах.

![[Pasted image 20250510134330.png]]

#### Манхэттенская метрика

$$ρ(x,y)=∑_i∣x_i−y_i∣​$$∣

Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты, скорее всего, очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далёкими друг от друга. Этого недостатка лишена манхэттенская метрика — в ней вместо квадрата используется модуль.

#### Метрика Минковского

$$ρ(x,y)=\left(∑_i∣x_i−y_i∣^p\right)^{1/p}$$

Является обобщением евклидовой (p=2) и манхэттенской (p=1) метрик.

#### Косинусное расстояние

$$ρ(x,y)=1−cos⁡θ=1−\frac{x⋅y}{∥x∥∥y∥}$$
Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно косинусное расстояние.
#### Расстояние Жаккара

$$ρ(A,B)=1−\frac{∣A∩B∣}{∣A∪B∣}$$
Его стоит использовать, если исследуемые объекты — это некоторые множества. Это полезно тем, что нет нужды придумывать векторные представления для этих множеств, чтобы использовать традиционные метрики.

#### Взвешенный KNN

У оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далёких. Можно заметить, что все индикаторы в формуле (2)(2) учитываются в сумме с одинаковыми коэффициентами. Возникает идея — назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу: ![[Pasted image 20250510135741.png]]
![[Pasted image 20250510135958.png]]
![[Pasted image 20250510140125.png]]
### Преимущества и недостатки

Сперва поговорим о преимуществах алгоритма.

- Непараметрический, то есть не делает явных предположений о распределении данных.
- Очень простой в объяснении и интерпретации.
- Достаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy.
- Может быть использован как для классификации, так и для регрессии.

Несмотря на большие преимущества, алгоритм не лишён и минусов.

- Неэффективный по памяти, поскольку нужно хранить всю обучающую выборку.
- Вычислительно дорогой по той же причине.
- Чувствителен к масштабу данных, а также к неинформативным признакам.
- Для применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако, если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления.

### Применение

Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них:

- Рекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна.
- Поиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи.
- Поиск аномалий и выбросов. Из-за того что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел.
- Задача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи.