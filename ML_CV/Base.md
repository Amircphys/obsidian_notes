## Base 

**Свертка** - опреация, при которой ядро свертки "скользит" по входному сигналу, в каждой точке приложения ядра вычисляется скалярное произведение, результат записывается в выходной сигнал. **Размерность свертки** = количество направлений, вдоль которых   движется ядро свертки, проходя по сигналу.
  
  2D-свертки с разными ядрами используются в обработке  изображений:
     - Сглаживание
     - Выделение границ и углов
     - Удаление шумов
На самом деле, рассмотренная операция называется не  сверткой, а кросс корреляцией. Разница в том, что при свертке ядро должно транспонироваться. Для математиков различие играет роль, для обучателей нейросетей - нет

● В области обработки изображений используются свертки с явно  заданными ядрами.
● Что получится, если сделать веса ядер свертки обучаемыми?  - **сверточный слой**

[Ссылка на анимации](https://github.com/vdumoulin/conv_arithmetic)
Сколько всего обучаемых весов у сверточного слоя c количесвтом фильтров K, ядром (3, 3) если размер входного тензора HxWxD?
     - 3x3xD (weights) + 1 (bias) - на 1 ядро
     - Всего фильтров K ⇒ весов Kx(3x3xD+1)

  Размер выходного тензора: $$H_{out} = \frac{H_{in}-KernelSize+2Padding}{Stride} + 1$$
  **Сверточный слой - backprop**:
	  ![[Pasted image 20241102221008.png]]
  ![[Pasted image 20241102221031.png]]
  ![[Pasted image 20241102221125.png]]
  Градиент по весам сверточного слоя = свертка входного   сигнала с градиентом по выходу слоя (в одномерном случае) - [ссылка](https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509)


**Pooling**. Обычно ширина сверточных слоев растет по мере увеличения  глубины сети, например, в сети ResNet18 количество сверток в одном слое растет так: 64 - 128 - 256 - 512. Чем больше размер тензоров, тем больше потребление памяти. Слой **Pooling** позволяет уменьшать H/W тензоров. 
	● Разобьем тензор на части по ширине и высоте  
	● В каждой части независимо от других посчитаем 1 статистику  (среднее - AveragePooling, максимум - MaxPooling)  
	● Склеим полученные статистики обратно в тензор  
	● Получится тензор прежней глубины, но меньшего размера по   ширине и высоте ● Можно посчитать статистики сразу по   всему каналу, а не по регионам  
	● В результате получится тензор размера  1х1хD, который можно отправить в    полносвязный слой - это будет **Global Pooling** 
	● Во время **обратного распространения** ошибки градиент передаётся только по тем ячейкам, которые были задействованы при вычислении результата (в случае максимума это только максимальный элемент).

**CNN** = convolutional neural network. Типичная структура CNN:
	((сверточный слой ⇒ активация) * k ⇒ пулинг) * m ⇒ линейный слой
	● В современных архитектурах как правило используются   свертки 3х3  
	● Первый сверточный слой видит "сырые" данные - его веса худо-бедно "понятны"  
	● Чем глубже в сеть, тем более абстрактным становится   представление исходного объекта  

#### Cвёртки
Cвёртки собирают информацию с некоторой области, которую можно по-разному интерпретировать в зависимости от вида ядра, например, судить, есть ли в данном фрагменте изображения вертикальные линии. В качестве ещё одной модификации процесса свёртки предлагается расширение области действия ядра, но не за счёт фактического увеличения размера ядра, а за счёт увеличения покрываемой области.

##### Свёртка 1 × 1 
Cвёртка 1 × 1 используется для уменьшения количества каналов при введении нелинейности. Этот трюк особенно полезен при построении глубоких и очень глубоких сетей, многие знаменитые архитектуры использовали его для сокращения количества вычислений. Дело в том, что при применении свёртки к слишком глубокому тензору (например, Ĉ = 256), мы можем предварительно уменьшить его глубину с помощью свёртки 1 × 1. Таким образом мы можем сохранить глубину самой сети, при этом на порядки уменьшая количество обучаемых параметров.

##### Разреженная связность
В слоях традиционной нейронной сети применяется умножение на матрицу параметров, в которой взаимодействие между каждым входным и каждым выходным элементами описывается отдельным параметром. Это означает, что каждый выходной элемент взаимодействует с каждым входным элементом. В свёрточных сетях же взаимодействия обычно разреженные (это свойство называют ещё разреженной связностью). ![[Pasted image 20241103105649.png]]


##### Смысловая интерпретация свёрточных сетей  
При использовании свёрток информация собирается не со всего объекта целиком, а ограниченными участками. Размер каждого такого участка характеризуется размером и способом наложения ядра этой свёртки. Соответственно, в каждой свёртке обученной нейронной сети хранится конкретная информация об областях изображения (если мы работаем изначально с изображением). Повторив операцию свёртки, мы получим уже более высокоуровневое представление изображения. То есть, если первая свёртка несла в себе информацию о расположении прямых линий, то вторая уже будет содержать информацию об изогнутых. Повторяя процесс свёртки многократно, мы будем получать сведения о все более сложных абстракциях. Процесс свёртки помогает перейти от конкретных деталей к более абстрактным. На последнем шаге мы будем получать высокоуровневые представления. Например, от вопроса, есть ли на изображении вертикальные линии, постепенно мы перейдём к вопросу, есть ли на изображении машина или медведь. 

![[Pasted image 20241103110133.png]]
Немаловажным процессом является пулинг. Его польза обусловлена тем, что он
уменьшает размеры тензоров, с которым ведутся операции, и процесс уменьшения
размера происходит на каждом канале независимо друг от друга.
Отметим ещё одну важную мысль: использование, например, **MaxPooling** идей-
но меняет принцип работы нейросети. Эта операция выбирает из некоторой обла-
сти максимальное значение, тем самым акцентирует внимание не на расположении
какого-либо объекта на изображении, а на самом факте наличия этого объекта. В
итоге мы получаем информацию о содержании изображения.

##### Пространственные свёртки (Spatial Separable Convolutions)
Основная идея заключается в представлении исходного ядра в виде произведения двух векторов. В этом случае мы будем применять две свёртки вместо одной, но это значительно сократит количество обучаемых параметров (6 против 9 в случае 3×3 ядра). Отсюда следует и название метода, каждый вектор соответствует размерностям Ĥ и Ŵ . Важнейшим недостатком этого метода является ограниченность класса матриц, представимых подобным образом.
![[Pasted image 20241103110907.png]]
##### Свёртка по каналам с объединением (Depth-wise Separable Convolution)
В этом методе тензор делится на каналы, каждый из которых будет обраба-
тываться своим ядром независимо от других. Полученные свёртки конкатени-
руются и к нему применяется свертка  1 × 1
![[Pasted image 20241103111459.png]]Предположим, что наш обучающий набор содержит изображения RGB размером 15x15 пикселей (3 канала!) и что мы используем 10 ядер размером 3x3x3 применяя обычную свертку. При одной свертке одного входного изображения (т. е. 3x3x3 слайда по первым 3x3x3 пикселям вашего RGB-изображения) вам придется выполнить 3x3x3 = 27 умножений, чтобы найти первое скалярное значение. Однако мы решили использовать 10 ядер, поэтому для первых 3x3 пикселей вашего изображения у нас будет 270 умножений. Поскольку мы не используем padding, ядру придется сделать 13 шагов (15-3+1 = 13), как по горизонтали, так и по вертикали. Следовательно, на одно изображение нам придется сделать 270 x 13 x 13 = 45630 умножений.
Применяя же Depth-wise Separable Convolution нам нужно сделать 13x13х3х3x3х1 + 13x13х10х1x1х3=9633 умножений.