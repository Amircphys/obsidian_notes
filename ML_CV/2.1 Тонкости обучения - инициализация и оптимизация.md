
## Инициализируем правильно
Казалось бы, инициализация параметров слоя нулями — это достаточно просто и лаконично. Но инициализация нулём (как и любой другой константой) ведёт к катастрофе! ![[Pasted image 20250624104733.png]]
Стоит, впрочем, отметить, что из-за численных ошибок значения параметров могут всё-таки сдвинуться с мёртвой точки, и тогда нейросеть что-нибудь выучит: ![[Pasted image 20250624104907.png]]
Perhaps the only property known with complete certainty is that the initial parameters need to “break symmetry” between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.
#### Эвристический подход №1: инициализация случайными числами
Если константная инициализация не подходит, можно инициализировать нейросеть случайными числами. Допустим, веса пришли из распределения с нулевым средним и дисперсией $σ^2$, например, из нормального распределения $N(0,σ^2)$. Рассмотрим результат дисперсии на выходе из линейного слоя: 
![[Pasted image 20250625093043.png]]
![[Pasted image 20250625093106.png]]
#### Подход №2: Xavier & Normalized Xavier initialization
Если обратиться к предыдущему подходу, можно обнаружить, что все выкладки верны как для «прямого» прохода (forward propagation), так и для обратного (backward propagation). Дисперсия градиента при этом меняется в $n_{out}Var(w)$ раз, где $n_{out}$​ — размерность следующего за $X$ промежуточного представления. И если мы хотим, чтобы сохранялись дисперсии и промежуточных представлений, и градиентов, у нас возникают сразу два ограничения: 
$$∀i,Var(w_i​)=1/n_{in}​​$$
$$∀i,Var(w_i​)=1/n_{out}​​$$
Легко заметить, что оба этих ограничения могут быть выполнены только в случае, когда размерность пространства не меняется при отображении, что случается далеко не всегда.
В [работе](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) Understanding the difficulty of training deep feedforward neural networks за авторством Xavier Glorot и Yoshua Bengio в качестве компромисса предлагается использовать параметры из распределения с дисперсией 
$$∀i,Var(w_i​)=\frac{2}{n_{in} + n_{out}}​​$$
![[Pasted image 20250625094536.png]]
#### Подход №3: Kaiming initialization

Xavier initialization во многом опиралась на поведение функции активации `tanh`. Данный тип инициализации и впрямь лучше подходит для нее, но само использование гиперболического тангенса приводит к некоторым сложностям (например, к затуханию градиентов).
![[Pasted image 20250625095811.png]]

## Регуляризация 

### Регуляризация через функцию потерь
В глубинном обучении часто используется техника Weight Decay, очень близкая к регуляризации Тихонова. Она представляет собой аналогичный штраф за высокие значения весов нейронной сети с коэффициентом регуляризации λ: ![[Pasted image 20250625151400.png]]
Также достаточно часто в качестве регуляризационного члена встречается энтропия распределения, предсказанного нейронной сетью. Представьте, что вы рекомендуете пользователю товары по истории его взаимодействия с сервисом, семплируя товары для показа в соответствии с распределением предсказанной релевантности. Вам может быть важно, чтобы рекомендации не были фиксированными (менялись при обновлении страницы), ведь это повысит вероятность того, что пользователь найдёт что-то интересное, а вы узнаете о нём что-нибудь новое. В такой ситуации при обучении модели вы можете потребовать, чтобы распределение предсказаний не сходилось к вырожденному, и в качестве дополнительной штрафной функции может выступать энтропия этого распределения. Энтропия дифференцируема, как и сами предсказанные величины, и может быть использована в качестве регуляризационного члена. Для задачи классификации он будет выглядеть следующим образом:
![[Pasted image 20250625151505.png]]
### Регуляризация через ограничение структуры модели
Внесение подходящих преобразований в структуру сети также может быть хорошим способом добиться желаемых результатов. Огромное влияние на развитие нейронных сетей оказали техники [dropout (2014)](https://jmlr.org/papers/v15/srivastava14a.html) и [batch normalization (2015)](https://arxiv.org/abs/1502.03167), позволившие сделать нейронные сети более устойчивыми к переобучению и многократно ускорить их сходимость соответственно.  

**Dropout**
Для этого можно было бы случайным образом «выключать» доступ к некоторым координатам внутренних представлений на этапе обучения. Тогда при выключении «полезных» координат произойдёт резкое изменение предсказаний модели, что приведёт к увеличению ошибки, а полученные градиенты этой ошибки укажут, как её исправить с использованием (и изменением) других координат. Сравнение тока информации по исходной модели и по модели с «выключенными» координатами внутренних представлений можно проиллюстрировать с помощью классической картинки: ![[Pasted image 20250625151754.png]]
![[Pasted image 20250625152007.png]]
Полезно провести аналогию с другим алгоритмом, использующим техники ансамблирования и метод случайных подпространств: речь о случайном лесе (Random Forest). При обучении сети на каждом шаге обучается лишь некоторая подсеть (некоторый подграф вычислений из исходного графа). При переходе в режим inference (то есть применения к реальным данным с целью получения результата, а не обучения) активируются сразу все подсети, и их результаты усредняются. Таким образом, сеть с dropout можно рассматривать как ансамбль из экспоненциально большого числа сетей меньшего размера (подробнее можно прочитать [здесь](https://arxiv.org/abs/1706.06859)). Это приводит к получению более устойчивой оценки значений целевой переменной.

В этом свойстве кроется и главное коварство dropout (как и большинства других техник регуляризации): благодаря получению более устойчивой оценки целевой переменной путём усреднения множества подсетей, эффективная обобщающая способность итоговой сети снижается! В самом деле, пусть при обучении каждый раз модели была доступна лишь половина параметров. В таком случае итоговая модель представляет собой усреднение множества более слабых моделей, в которых вдвое меньше параметров. Её предсказания будут более устойчивы к шуму, но при этом она неспособна выучить столь сложные зависимости, как сеть аналогичной структуры, но без dropout. То есть за более устойчивые предсказания (и получение менее переобученной модели) приходится расплачиваться и меньшей обобщающей способностью.

**Batch normalization**
Появление техники batch normalization привело к значительному ускорению обучения нейронных сетей. Использование batch normalization гарантирует, что каждая компонента представления на выходе будет иметь контролируемое среднее и дисперсию. Достигается это следующим образом: ![[Pasted image 20250625152738.png]]
![[Pasted image 20250625152802.png]]
Причина популярности batch normalization заключается в значительном ускорении обучения нейронных сетей и в улучшении их сходимости в целом.
