

Предположим, мы решаем задачу классификации с **_l_** классами, наш алгоритм выдаёт некоторые оценки принадлежности объектов к классам – уверенности (Confidences), после чего относит объект к классу с максимальной оценкой (т.е. выдаёт ответ, в котором максимально уверен): 
![[Pasted image 20250707160722.png]]
Возникает естественный вопрос: **какова вероятность, что ответ верный?** Нам бы хотелось уметь оценивать её на этапе формирования ответа. Напрашивающийся и самый «удобный» вариант – **эта вероятность равна максимальной оценке (уверенности):** 
![[Pasted image 20250707160806.png]]
Если равенство выполняется с достаточной точностью, то говорят, что «**классификатор (хорошо) откалиброван**». Может быть и более жёсткое условие калибровки – чтобы все оценки соответствовали вероятностям:
![[Pasted image 20250707160858.png]]
для всех **_k_**. Например, если классификатор по классам «кошка», «собака», «суслик» получил уверенности (0.8, 0.2, 0.0), то с вероятностью 0.8 правильный класс «кошка», а с вероятностью 0.2 – «собака»  и точно объект не может принадлежать классу «суслик».

Опишем, как на практике оценить откалиброванность алгоритма. Надо сравнить **confidence (уверенность)** и **точность (accuracy)** на тестовой выборке: $b_k(x)$ vs $P(y(x)=a(x))$. **Понятно, что если для большой группы объектов алгоритм выдал оценку 0.8 за класс «собака», то в случае «хорошей калибровки» среди них будет примерно 80% объектов этого класса. На практике проблема в том, что вряд ли для большой группы объектов ответ будет именно 0.8 (а не 0.81, 0.8032 и т.д.) Поэтому сравнивают, разделив объекты на группы – «бины» (bins)**:
![[Pasted image 20250707220305.png]]
Бины часто выбирают с границами [0, 0.1], [0.1, 0.2] и т.д. На рис. 1 розовые столбцы – полученные средние уверенности, а синие – точности – в соответствующих бинах. Чёрным показана доля объектов выборки, попавших в соответствующий бин. Понятно, что чем больше доля, тем надёжнее оценки уверенности и точности. В случае, если синий столбец выше розового, говорят о недостаточной уверенности (under-confident), если ниже – о **чрезмерной уверенности или «пере-уверенности» (over-confident)**.

**Современные нейронные сети (НС) являются неоткалиброванными!** Чаще они страдают «пере-уверенностью» (over-confident). Во-первых, это происходит потому, что при обучении НС на отложенных данных следят за точностью (accuracy), не обращая внимание на уверенность. Современные сети очень сложны, что приводит к переобучению (даже если на обучающей выборке NLL убывает при обучении сети, на тестовой выборке NLL может возрастать. 

В-третьих, большинство современных приёмов, используемых в DL (большая глубина, нормализация по батчам и т.п.) ухудшают калибровку, см. рис. ниже.

Методы калибровки - выполняют обычно на отложенной выборке. Большинство методов подменяют ответ алгоритма, точнее уверенность ответа: смещённую оценку заменяют на значение, которое ближе к точности. Функцию, которая осуществляет такую подмену, называют **функцией деформации**.

