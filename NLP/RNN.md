## LSTM
LSTM – это рекуррентная нейросеть, то есть нейросеть, которая работает с объектами (текстом, действиями пользователя или чем-то другим) последовательно. Такие сети состоят из цепочки одинаковых блоков, и при обработке очередного токена обращаются к предыдущим, как к контексту.
![[Pasted image 20250320155838.png]]
- **Скрытое состояние** (**h_t**). Это внутренняя память сети, которая передается от слоя к слою.
- **Состояние ячейки** (**c_t**). Это внутренняя информации LSTM-блока, с помощью которой формируется скрытое состояние.
- **Гейты**, с помощью которых мы контролируем, какую информацию мы оставляем или удаляем из памяти. Их будет три: гейт забывания, гейт входного состояния и гейт выходного состояния.

1. Перво-наперво блок должен на основе предыдущего скрытого состояния $h_{t-1}$  и нового поступившего токена $x_{t}$ "решить", какую информацию из предыдущего состояния ячейки $C_{t-1}$ он пропустит дальше, а какую забудет. Для этого отрабатывает так называемый ****"гейт забывания"****. Он состоит всего из одного сигмоидального слоя, который сопоставляет каждой компоненте вектора информации число от 0 до 1, где 1 – это "пропустить полностью", а 0 - "забыть полностью" (см. формулу ниже). ![тдвдв](https://datasecrets.ru/media/LSTM3-focus-f.png)
2. Следующий шаг – решить, какую новую информацию из поступившего токена $x_{t}$ и предыдущего скрытого состояния $h_{t-1}$ мы добавим в состояние блока. Для этого открывается следующий гейт – **гейт входного состояния**. Здесь можно было бы добавить в $C_{t-1}$ обычную линейную комбинацию $x_t$ и $h_{t-1}$ к которой применена функция активации _****tanh****_ (cм. формулу 2 на картинке ниже). Но мы не уверены, что вся эта информация достаточно релевантна, и хотим взять только некоторую ее долю. Чтобы понять, какую именно, с помощью сигмоиды снова вычисляется вектор "забывания", который состоит из чисел от 0 до 1 (cм. формулу 1 на картинке ниже). ![вмвмв](https://datasecrets.ru/media/LSTM3-focus-i.png)
3. Вычисляем новое состояние ячейки $C_{t}$. После применения гейта забывания и гейта входного состояния оно будет равно сумме произведений сигмоидальных векторов $f_{t}$ на информацию $C_{t-1}$ и $\hat{C_t}$ (см. формулу ниже).![вмвмв](https://datasecrets.ru/media/LSTM3-focus-C.png)
4. Осталось только одно – вычислить скрытое состояние ****h********t****, которое играет роль выходного вектора LSTM-блока. Оно вычисляется из только что сформированного сетью состояния ячейки $C_t$ с помощью **гейта выходного состояния**. Работает аналогично другим гейтам: у нас есть $C_{t}$, к которому мы применили функцию активации _****tanh****_, и на основе $x_{t}$ и $h_{t-1}$ мы составляем сигмоидальный вектор (см. формулу 1 внизу) чтобы решить, какую часть информации из _****tanh(C****__****t****__****)****_ мы отнесем в скрытое состояние ****h********t**** (см. формулу 2 внизу). ![вмвмвв](https://datasecrets.ru/media/LSTM3-focus-o.png)

## GRU
![[Pasted image 20250320162505.png]]
![[Pasted image 20250320162927.png]]

